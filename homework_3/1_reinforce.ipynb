{"cells":[{"cell_type":"markdown","metadata":{"id":"tsmoX_o4zd2H"},"source":["# REINFORCE in pytorch (5 pts)\n","\n","Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"vtHwhL_szd2P","executionInfo":{"status":"ok","timestamp":1653939194835,"user_tz":-180,"elapsed":6072,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","    !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"JtleWj_Gzd2R","executionInfo":{"status":"ok","timestamp":1653939197897,"user_tz":-180,"elapsed":618,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}},"outputId":"ebadf5a2-3f2e-413c-991d-c7135b9038aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f9dda9a7810>"]},"metadata":{},"execution_count":38},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import gym\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make(\"CartPole-v0\").env\n","env.reset()\n","\n","plt.imshow(env.render(\"rgb_array\"))"]},{"cell_type":"markdown","metadata":{"id":"WbmF3L7Qzd2S"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"hljPy9lczd2T"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"iCT5d8zxzd2T","executionInfo":{"status":"ok","timestamp":1653939197898,"user_tz":-180,"elapsed":34,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"aOWTFep3zd2U","executionInfo":{"status":"ok","timestamp":1653939197900,"user_tz":-180,"elapsed":35,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["# Build a simple neural network that predicts policy logits. \n","# Keep it simple: CartPole isn't worth deep architectures.\n","\n","state_dim = env.observation_space.shape[0]\n","n_actions = env.action_space.n\n","\n","model = nn.Sequential(\n","  nn.Linear(state_dim, 250),\n","  nn.ReLU(),\n","  nn.Linear(250, 250),\n","  nn.ReLU(),\n","  nn.Linear(250, n_actions)\n",")"]},{"cell_type":"markdown","metadata":{"id":"2GpDHaBEzd2V"},"source":["#### Predict function"]},{"cell_type":"markdown","metadata":{"id":"KUUVpt7-zd2W"},"source":["Note: output value of this function is not a torch tensor, it's a numpy array.\n","So, here gradient calculation is not needed.\n","<br>\n","Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n","to suppress gradient calculation.\n","<br>\n","Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n","<br>\n","With `.detach()` computational graph is built but then disconnected from a particular tensor,\n","so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n","<br>\n","In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"V5QfZm4rzd2X","executionInfo":{"status":"ok","timestamp":1653939197901,"user_tz":-180,"elapsed":36,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    # convert states, compute logits, use softmax to get probability\n","    states = Variable(torch.FloatTensor(states))\n","    probs = F.softmax(model.forward(states), dim=1)\n","    return probs.detach().numpy()"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"Gc2eEu6pzd2Z","executionInfo":{"status":"ok","timestamp":1653939197902,"user_tz":-180,"elapsed":36,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["test_states = np.array([env.reset() for _ in range(5)])\n","test_probas = predict_probs(test_states)\n","assert isinstance(\n","    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (\n","    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n","assert np.allclose(np.sum(test_probas, axis=1),\n","                   1), \"probabilities do not sum to 1\""]},{"cell_type":"markdown","metadata":{"id":"HM1P_Nxbzd2a"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"g9ogylMbzd2a","executionInfo":{"status":"ok","timestamp":1653939197903,"user_tz":-180,"elapsed":36,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    play a full session with REINFORCE agent and train at the session end.\n","    returns sequences of states, actions andrewards\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(np.array([s]))[0]\n","\n","        # Sample action with given probabilities.\n","        a = np.random.choice(n_actions, p = action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"XFFDim3czd2c","executionInfo":{"status":"ok","timestamp":1653939197905,"user_tz":-180,"elapsed":37,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["# test it\n","states, actions, rewards = generate_session(env)"]},{"cell_type":"markdown","metadata":{"id":"O0rDiskNzd2d"},"source":["### Computing cumulative rewards"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"1T8Y8LOQzd2d","executionInfo":{"status":"ok","timestamp":1653939197906,"user_tz":-180,"elapsed":35,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    take a list of immediate rewards r(s,a) for the whole session \n","    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    The simple way to compute cumulative rewards is to iterate from last to first time tick\n","    and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    gammas = list()\n","    gammas.append(rewards[-1])\n","    \n","    for reward in rewards[-2::-1]:\n","        gammas.append(reward + gamma * gammas[-1])\n","    \n","    return gammas[::-1]"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OUusJMKezd2f","executionInfo":{"status":"ok","timestamp":1653939197907,"user_tz":-180,"elapsed":34,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}},"outputId":"f5a52e3a-9bb8-4192-c1cc-f407c2fdfd54"},"outputs":[{"output_type":"stream","name":"stdout","text":["looks good!\n"]}],"source":["get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n","                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(get_cumulative_rewards(\n","    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(get_cumulative_rewards(\n","    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"]},{"cell_type":"markdown","metadata":{"id":"yeTaLTbYzd2f"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over T } \\sum_{i=1}^T  G(s_i,a_i) $$\n","\n","\n","Following the REINFORCE algorithm, we can define our objective as follows: \n","\n","$$ \\hat J \\approx { 1 \\over T } \\sum_{i=1}^T \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G(s_i,a_i) $$\n","\n","Entropy Regularizer\n","  $$ H = - {1 \\over T} \\sum_{i=1}^T  \\sum_{a \\in A} {\\pi_\\theta(a|s_i) \\cdot \\log \\pi_\\theta(a|s_i)}$$\n","\n","$T$ is session length\n","\n","So we optimize a linear combination of $- \\hat J$, $-H$\n","\n","When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"Ql6CfB2Jzd2g","executionInfo":{"status":"ok","timestamp":1653939197908,"user_tz":-180,"elapsed":29,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["def to_one_hot(y_tensor, ndims):\n","    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n","    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n","    y_one_hot = torch.zeros(\n","        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n","    return y_one_hot"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"OLaCcIk2zd2h","executionInfo":{"status":"ok","timestamp":1653939197909,"user_tz":-180,"elapsed":29,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["# Your code: define optimizers\n","from torch import optim\n","optimizer = optim.Adam(model.parameters())\n","\n","\n","def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n","    \"\"\"\n","    Takes a sequence of states, actions and rewards produced by generate_session.\n","    Updates agent's weights by following the policy gradient above.\n","    Please use Adam optimizer with default parameters.\n","    \"\"\"\n","\n","    # cast everything into torch tensors\n","    states = torch.tensor(states, dtype=torch.float32)\n","    actions = torch.tensor(actions, dtype=torch.int32)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n","\n","    # predict logits, probas and log-probas using an agent.\n","    logits = model(states)\n","    probs = nn.functional.softmax(logits, -1)\n","    log_probs = nn.functional.log_softmax(logits, -1)\n","\n","    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n","        \"please use compute using torch tensors and don't use predict_probs function\"\n","\n","    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n","    log_probs_for_actions = torch.sum(\n","        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n","   \n","    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n","    entropy = -(probs * log_probs).mean() * entropy_coef\n","    loss = -torch.mean(log_probs_for_actions * cumulative_returns) - entropy\n","\n","    # Gradient descent step\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # technical: return session rewards to print them later\n","    return np.sum(rewards)"]},{"cell_type":"markdown","metadata":{"id":"b5a7JGq_zd2i"},"source":["### The actual training"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jy7Y1fzNzd2i","executionInfo":{"status":"ok","timestamp":1653939259163,"user_tz":-180,"elapsed":61282,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}},"outputId":"a8f34add-6fbd-4aaa-df21-ce04a0bc1ae3"},"outputs":[{"output_type":"stream","name":"stdout","text":["mean reward:13.320\n","mean reward:9.790\n","mean reward:11.580\n","mean reward:37.280\n","mean reward:78.810\n","mean reward:134.090\n","mean reward:150.600\n","mean reward:256.890\n","mean reward:181.180\n","mean reward:142.960\n","mean reward:246.120\n","mean reward:591.540\n","You Win!\n"]}],"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env))\n","               for _ in range(100)]  # generate new sessions\n","    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n","    if np.mean(rewards) > 500:\n","        print(\"You Win!\")  # but you can train even further\n","        break"]},{"cell_type":"markdown","metadata":{"id":"3LgHuqrBzd2j"},"source":["### Video"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"jnLGHH4nzd2j","executionInfo":{"status":"ok","timestamp":1653939264548,"user_tz":-180,"elapsed":5390,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["# record sessions\n","import gym.wrappers\n","monitor_env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n","                           directory=\"videos\", force=True)\n","sessions = [generate_session(monitor_env) for _ in range(100)]\n","monitor_env.close()"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"resources":{"http://localhost:8080/videos/openaigym.video.1.59.video000027.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":501},"id":"KjwkA5f0zd2k","executionInfo":{"status":"ok","timestamp":1653940910761,"user_tz":-180,"elapsed":405,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}},"outputId":"f9d37f5e-c70a-44ef-ec04-ad6335993150"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"./videos/openaigym.video.1.59.video000027.mp4\" type=\"video/mp4\">\n","</video>\n"]},"metadata":{},"execution_count":56}],"source":["# show video\n","from IPython.display import HTML\n","import os\n","\n","video_names = list(\n","    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(\"./videos/\" + video_names[0]))  # this may or may not be the _last_ video. Try other indices"]},{"cell_type":"markdown","source":["В ноутбуке не отобразилось ни одно видео, поэтому добавил всю папку целиком"],"metadata":{"id":"W7cns9Ohjk-E"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"1_reinforce.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}