{"cells":[{"cell_type":"markdown","metadata":{"id":"3EkYnYltvG7J"},"source":["This notebook is based on<br>https://github.com/yandexdataschool/Practical_RL/blob/master/week03_model_free/seminar_qlearning.ipynb and<br>https://github.com/yandexdataschool/Practical_RL/blob/master/week03_model_free/homework.ipynb"]},{"cell_type":"markdown","metadata":{"id":"d8pKXCm3vG7R"},"source":["## Q-learning\n","\n","This notebook will guide you through implementation of vanilla Q-learning algorithm.\n","\n","You need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3A9SVIDvG7T","executionInfo":{"status":"ok","timestamp":1649827696286,"user_tz":-180,"elapsed":28786,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}},"outputId":"dc8569f5-3c3d-477b-9516-8eb678dae210"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package xvfb.\n","(Reading database ... 155455 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Starting virtual X frame buffer: Xvfb.\n"]}],"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n","\n","    !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"lnic63yKvG7W","executionInfo":{"status":"ok","timestamp":1649827696588,"user_tz":-180,"elapsed":313,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"QpFt38NxvG7Y","executionInfo":{"status":"ok","timestamp":1649828932967,"user_tz":-180,"elapsed":419,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["from collections import defaultdict\n","import random\n","import math\n","import numpy as np\n","\n","\n","class QLearningAgent:\n","    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n","        \"\"\"\n","        Q-Learning Agent\n","        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n","        Instance variables you have access to\n","          - self.epsilon (exploration prob)\n","          - self.alpha (learning rate)\n","          - self.discount (discount rate aka gamma)\n","\n","        Functions you should use\n","          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n","            which returns legal actions for a state\n","          - self.get_qvalue(state,action)\n","            which returns Q(state,action)\n","          - self.set_qvalue(state,action,value)\n","            which sets Q(state,action) := value\n","        !!!Important!!!\n","        Note: please avoid using self._qValues directly. \n","            There's a special self.get_qvalue/set_qvalue for that.\n","        \"\"\"\n","\n","        self.get_legal_actions = get_legal_actions\n","        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.discount = discount\n","\n","    def get_qvalue(self, state, action):\n","        \"\"\" Returns Q(state,action) \"\"\"\n","        return self._qvalues[state][action]\n","\n","    def set_qvalue(self, state, action, value):\n","        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n","        self._qvalues[state][action] = value\n","\n","    #---------------------START OF YOUR CODE---------------------#\n","\n","    def get_value(self, state):\n","        \"\"\"\n","        Compute your agent's estimate of V(s) using current q-values\n","        V(s) = max_over_action Q(state,action) over possible actions.\n","        Note: please take into account that q-values can be negative.\n","        \"\"\"\n","        possible_actions = self.get_legal_actions(state)\n","\n","        # If there are no legal actions, return 0.0\n","        if len(possible_actions) == 0:\n","            return 0.0\n","\n","        value = np.max([self.get_qvalue(state, action) for action in possible_actions])\n","\n","        return value\n","\n","    def update(self, state, action, reward, next_state):\n","        \"\"\"\n","        You should do your Q-Value update here:\n","           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n","        \"\"\"\n","\n","        # agent parameters\n","        gamma = self.discount\n","        learning_rate = self.alpha\n","\n","        new_value = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate * (reward + gamma * self.get_value(next_state))\n","\n","        self.set_qvalue(state, action, new_value)\n","\n","    def get_best_action(self, state):\n","        \"\"\"\n","        Compute the best action to take in a state (using current q-values). \n","        \"\"\"\n","        possible_actions = self.get_legal_actions(state)\n","\n","        # If there are no legal actions, return None\n","        if len(possible_actions) == 0:\n","            return None\n","\n","        best_action_idx = np.argmax([self.get_qvalue(state, action) for action in possible_actions])\n","\n","        best_action = possible_actions[best_action_idx]\n","\n","        return best_action\n","\n","    def get_action(self, state):\n","        \"\"\"\n","        Compute the action to take in the current state, including exploration.  \n","        With probability self.epsilon, we should take a random action.\n","            otherwise - the best policy action (self.get_best_action).\n","\n","        Note: To pick randomly from a list, use random.choice(list). \n","              To pick True or False with a given probablity, generate uniform number in [0, 1]\n","              and compare it with your probability\n","        \"\"\"\n","\n","        # Pick Action\n","        possible_actions = self.get_legal_actions(state)\n","        action = None\n","\n","        # If there are no legal actions, return None\n","        if len(possible_actions) == 0:\n","            return None\n","\n","        # agent parameters:\n","        epsilon = self.epsilon\n","\n","        rng = np.random.default_rng()\n","        random_u = rng.uniform(0,1)\n","        action = rng.choice(possible_actions)\n","        chosen_action = action if random_u <= epsilon else self.get_best_action(state)\n","\n","        return chosen_action"]},{"cell_type":"markdown","metadata":{"id":"zQdEmDt8vG7c"},"source":["### Try it on taxi\n","\n","Here we use the qlearning agent on taxi env from openai gym.\n","You will need to insert a few agent functions here."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"DrF5wx4AvG7d","executionInfo":{"status":"ok","timestamp":1649828934779,"user_tz":-180,"elapsed":388,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["import gym\n","env = gym.make(\"Taxi-v3\")\n","\n","n_actions = env.action_space.n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ipXvrLdBvG7e","executionInfo":{"status":"ok","timestamp":1649828936127,"user_tz":-180,"elapsed":4,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["agent = QLearningAgent(\n","    alpha=0.5, epsilon=0.25, discount=0.99,\n","    get_legal_actions=lambda s: range(n_actions))"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"mdTkCggcvG7g","executionInfo":{"status":"ok","timestamp":1649829014221,"user_tz":-180,"elapsed":274,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}}},"outputs":[],"source":["def play_and_train(env, agent, t_max=10**4):\n","    \"\"\"\n","    This function should \n","    - run a full game, actions given by agent's e-greedy policy\n","    - train agent using agent.update(...) whenever it is possible\n","    - return total reward\n","    \"\"\"\n","    total_reward = 0.0\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # get agent to pick action given state s.\n","        a = agent.get_action(s)\n","\n","        next_s, r, done, _ = env.step(a)\n","\n","        # train (update) agent for state s\n","        agent.update(s, a, r, next_s)\n","\n","        s = next_s\n","        total_reward += r\n","        if done:\n","            break\n","\n","    return total_reward"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MNi_zZ1wvG7i","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1649829024123,"user_tz":-180,"elapsed":6395,"user":{"displayName":"Михаил Евстифеев","userId":"06204896677496909612"}},"outputId":"5f84c73b-076a-4275-d13c-6d793be6d52b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["from IPython.display import clear_output\n","\n","rewards = []\n","for i in range(1000):\n","    rewards.append(play_and_train(env, agent))\n","    agent.epsilon *= 0.99\n","\n","    if i % 100 == 0:\n","        clear_output(True)\n","        plt.title('eps = {:e}, mean reward = {:.1f}'.format(agent.epsilon, np.mean(rewards[-10:])))\n","        plt.plot(rewards)\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"u0EWsJJbvG7k"},"source":["# Binarized state spaces\n","\n","Use agent to train efficiently on `CartPole-v0`. This environment has a continuous set of possible states, so you will have to group them into bins somehow.\n","\n","The simplest way is to use `round(x, n_digits)` (or `np.round`) to round a real number to a given amount of digits. The tricky part is to get the `n_digits` right for each state to train effectively.\n","\n","Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgo3-fksvG7k"},"outputs":[],"source":["def make_env():\n","    return gym.make('CartPole-v0').env  # .env unwraps the TimeLimit wrapper\n","\n","env = make_env()\n","n_actions = env.action_space.n\n","\n","print(\"first state: %s\" % (env.reset()))\n","plt.imshow(env.render('rgb_array'))"]},{"cell_type":"markdown","metadata":{"id":"DoVNovb9vG7l"},"source":["### Play a few games\n","\n","We need to estimate observation distributions. To do so, we'll play a few games and record all states."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4h7CfhhvG7m"},"outputs":[],"source":["def visualize_cartpole_observation_distribution(seen_observations):\n","    seen_observations = np.array(seen_observations)\n","    \n","    # The meaning of the observations is documented in\n","    # https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n","\n","    f, axarr = plt.subplots(2, 2, figsize=(16, 9), sharey=True)\n","    for i, title in enumerate(['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Velocity At Tip']):\n","        ax = axarr[i // 2, i % 2]\n","        ax.hist(seen_observations[:, i], bins=20)\n","        ax.set_title(title)\n","        xmin, xmax = ax.get_xlim()\n","        ax.set_xlim(min(xmin, -xmax), max(-xmin, xmax))\n","        ax.grid()\n","    f.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFmHv3H2vG7n"},"outputs":[],"source":["seen_observations = []\n","for _ in range(1000):\n","    seen_observations.append(env.reset())\n","    done = False\n","    while not done:\n","        s, r, done, _ = env.step(env.action_space.sample())\n","        seen_observations.append(s)\n","\n","visualize_cartpole_observation_distribution(seen_observations)"]},{"cell_type":"markdown","metadata":{"id":"PHKH2JQovG7o"},"source":["## Binarize environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3COVJ7xvG7p"},"outputs":[],"source":["from gym.core import ObservationWrapper\n","\n","\n","class Binarizer(ObservationWrapper):\n","    def observation(self, state):\n","        # Hint: you can do that with round(x, n_digits).\n","        # You may pick a different n_digits for each dimension.\n","        state = <YOUR CODE: round state to some amount digits>\n","\n","        return tuple(state)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvxX7sSavG7p"},"outputs":[],"source":["env = Binarizer(make_env())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_a7HfVBvG7q"},"outputs":[],"source":["seen_observations = []\n","for _ in range(1000):\n","    seen_observations.append(env.reset())\n","    done = False\n","    while not done:\n","        s, r, done, _ = env.step(env.action_space.sample())\n","        seen_observations.append(s)\n","        if done:\n","            break\n","\n","visualize_cartpole_observation_distribution(seen_observations)"]},{"cell_type":"markdown","metadata":{"id":"eW2jq6tyvG7r"},"source":["## Learn binarized policy\n","\n","Now let's train a policy that uses binarized state space.\n","\n","__Tips:__\n","\n","* Note that increasing the number of digits for one dimension of the observations increases your state space by a factor of $10$.\n","* If your binarization is too fine-grained, your agent will take much longer than 10000 steps to converge. You can either increase the number of iterations and reduce epsilon decay or change binarization. In practice we found that this kind of mistake is rather frequent.\n","* If your binarization is too coarse, your agent may fail to find the optimal policy. In practice we found that on this particular environment this kind of mistake is rare.\n","* **Start with a coarse binarization** and make it more fine-grained if that seems necessary.\n","* Having $10^3$–$10^4$ distinct states is recommended (`len(agent._qvalues)`), but not required.\n","* If things don't work without annealing $\\varepsilon$, consider adding that, but make sure that it doesn't go to zero too quickly.\n","\n","A reasonable agent should attain an average reward of at least 50."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KERtHpSXvG7s"},"outputs":[],"source":["import pandas as pd\n","\n","def moving_average(x, span=100):\n","    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVZvA8jTvG7t"},"outputs":[],"source":["agent = QLearningAgent(\n","    alpha=0.5, epsilon=0.25, discount=0.99,\n","    get_legal_actions=lambda s: range(n_actions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOP5wOjDvG7t"},"outputs":[],"source":["rewards = []\n","epsilons = []\n","\n","for i in range(10000):\n","    reward = play_and_train(env, agent)\n","    rewards.append(reward)\n","    epsilons.append(agent.epsilon)\n","    \n","    # OPTIONAL: <YOUR CODE: adjust epsilon>\n","\n","    if i % 100 == 0:\n","        rewards_ewma = moving_average(rewards)\n","        \n","        clear_output(True)\n","        plt.plot(rewards, label='rewards')\n","        plt.plot(rewards_ewma, label='rewards ewma@100')\n","        plt.legend()\n","        plt.grid()\n","        plt.title('eps = {:e}, rewards ewma@100 = {:.1f}'.format(agent.epsilon, rewards_ewma[-1]))\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTHn0WUXvG7u"},"outputs":[],"source":["print('Your agent has learned {} Q-values.'.format(len(agent._qvalues)))"]},{"cell_type":"markdown","metadata":{"id":"PWl5AcjAvG7u"},"source":["# SARSA"]},{"cell_type":"markdown","metadata":{"id":"QruA_gS8vG7v"},"source":["Now we gonna implement Expected Value SARSA on top of it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7YvT3DtvG7v"},"outputs":[],"source":["class EVSarsaAgent(QLearningAgent):\n","    \"\"\" \n","    An agent that changes some of q-learning functions to implement Expected Value SARSA. \n","    Note: this demo assumes that your implementation of QLearningAgent.update uses get_value(next_state).\n","    If it doesn't, please add\n","        def update(self, state, action, reward, next_state):\n","            and implement it for Expected Value SARSA's V(s')\n","    \"\"\"\n","\n","    def get_value(self, state):\n","        \"\"\" \n","        Returns Vpi for current state under epsilon-greedy policy:\n","          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n","\n","        Hint: all other methods from QLearningAgent are still accessible.\n","        \"\"\"\n","        epsilon = self.epsilon\n","        possible_actions = self.get_legal_actions(state)\n","\n","        # If there are no legal actions, return 0.0\n","        if len(possible_actions) == 0:\n","            return 0.0\n","\n","        <YOUR CODE: see docstring>\n","\n","        return state_value"]},{"cell_type":"markdown","metadata":{"id":"MT5rExBvvG7w"},"source":["### Cliff World\n","\n","Let's now see how our algorithm compares against q-learning in case where we force agent to explore all the time.\n","\n","<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/cliffworld.png width=600>\n","<center><i>image by cs188</i></center>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqA9At0YvG7x"},"outputs":[],"source":["import gym.envs.toy_text\n","env = gym.envs.toy_text.CliffWalkingEnv()\n","n_actions = env.action_space.n\n","\n","print(env.__doc__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcYAxBxyvG7x"},"outputs":[],"source":["# Our cliffworld has one difference from what's on the image: there is no wall.\n","# Agent can choose to go as close to the cliff as it wishes. x:start, T:exit, C:cliff, o: flat ground\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88b3HiRXvG7x"},"outputs":[],"source":["agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n","                           get_legal_actions=lambda s: range(n_actions))\n","\n","agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n","                          get_legal_actions=lambda s: range(n_actions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5KPhB4HvG7y"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","rewards_sarsa, rewards_ql = [], []\n","\n","for i in range(5000):\n","    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n","    rewards_ql.append(play_and_train(env, agent_ql))\n","    # Note: agent.epsilon stays constant\n","\n","    if i % 100 == 0:\n","        clear_output(True)\n","        print('EVSARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n","        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n","        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n","        plt.plot(moving_average(rewards_sarsa), label='ev_sarsa')\n","        plt.plot(moving_average(rewards_ql), label='qlearning')\n","        plt.grid()\n","        plt.legend()\n","        plt.ylim(-500, 0)\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"adUQ7uWSvG7z"},"source":["Let's now see what did the algorithms learn by visualizing their actions at every state."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5FGq5sFvG7z"},"outputs":[],"source":["def draw_policy(env, agent):\n","    \"\"\" Prints CliffWalkingEnv policy with arrows. Hard-coded. \"\"\"\n","    n_rows, n_cols = env._cliff.shape\n","\n","    actions = '^>v<'\n","\n","    for yi in range(n_rows):\n","        for xi in range(n_cols):\n","            if env._cliff[yi, xi]:\n","                print(\" C \", end='')\n","            elif (yi * n_cols + xi) == env.start_state_index:\n","                print(\" X \", end='')\n","            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n","                print(\" T \", end='')\n","            else:\n","                print(\" %s \" % actions[agent.get_best_action(yi * n_cols + xi)], end='')\n","        print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExAdetrRvG70"},"outputs":[],"source":["print(\"Q-Learning\")\n","draw_policy(env, agent_ql)\n","\n","print(\"SARSA\")\n","draw_policy(env, agent_sarsa)"]},{"cell_type":"markdown","metadata":{"id":"-LfbTc3uvG70"},"source":["Explain the results. Why the algorithms learned such policies?"]},{"cell_type":"markdown","metadata":{"id":"61ynjtdHvG71"},"source":["<< Your explanation >>"]},{"cell_type":"markdown","metadata":{"id":"E2arMTihvG71"},"source":["# Experience replay"]},{"cell_type":"markdown","metadata":{"id":"iOght-uGvG72"},"source":["There's a powerful technique that you can use to improve sample efficiency for off-policy algorithms: [spoiler] Experience replay :)\n","\n","The catch is that you can train Q-learning and EV-SARSA on `<s,a,r,s'>` tuples even if they aren't sampled under current agent's policy. So here's what we're gonna do:\n","\n","<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png width=480>\n","\n","#### Training with experience replay\n","1. Play game, sample `<s,a,r,s'>`.\n","2. Update q-values based on `<s,a,r,s'>`.\n","3. Store `<s,a,r,s'>` transition in a buffer. \n"," 3. If buffer is full, delete earliest data.\n","4. Sample K such transitions from that buffer and update q-values based on them.\n","\n","\n","To enable such training, first we must implement a memory structure that would act like such a buffer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTfXRIQZvG72"},"outputs":[],"source":["import random\n","\n","\n","class ReplayBuffer(object):\n","    def __init__(self, size):\n","        \"\"\"\n","        Create Replay buffer.\n","        Parameters\n","        ----------\n","        size: int\n","            Max number of transitions to store in the buffer. When the buffer\n","            overflows the old memories are dropped.\n","\n","        Note: for this assignment you can pick any data structure you want.\n","              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n","              However you may find out there are faster and/or more memory-efficient ways to do so.\n","        \"\"\"\n","        self._storage = []\n","        self._maxsize = size\n","\n","        # OPTIONAL: YOUR CODE\n","\n","    def __len__(self):\n","        return len(self._storage)\n","\n","    def add(self, state, action, reward, next_state, done):\n","        '''\n","        Make sure, _storage will not exceed _maxsize. \n","        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n","        '''\n","        data = (state, action, reward, next_state, done)\n","\n","        # add data to storage\n","        <YOUR CODE>\n","\n","    def sample(self, batch_size):\n","        \"\"\"Sample a batch of experiences.\n","        Parameters\n","        ----------\n","        batch_size: int\n","            How many transitions to sample.\n","        Returns\n","        -------\n","        states_batch: np.array\n","            batch of observations\n","        actions_batch: np.array\n","            batch of actions executed given obs_batch\n","        rewards_batch: np.array\n","            rewards received as results of executing act_batch\n","        next_states_batch: np.array\n","            next set of observations seen after executing act_batch\n","        done: np.array\n","            done_mask[i] = 1 if executing act_batch[i] resulted in\n","            the end of an episode and 0 otherwise.\n","        \"\"\"\n","        idxes = <YOUR CODE: randomly generate batch_size integers to be used as indexes of samples>\n","\n","        # collect <s,a,r,s',done> for each index\n","        <YOUR CODE>\n","\n","        return (\n","            np.array( <YOUR CODE: states> ),\n","            np.array( <YOUR CODE: actions> ),\n","            np.array( <YOUR CODE: rewards> ),\n","            np.array( <YOUR CODE: next_states> ),\n","            np.array( <YOUR CODE: done>,\n","        )"]},{"cell_type":"markdown","metadata":{"id":"aAKh72SCvG73"},"source":["Some tests to make sure your buffer works right"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqZxkZH1vG73"},"outputs":[],"source":["def obj2arrays(obj):\n","    for x in obj:\n","        yield np.array([x])\n","\n","def obj2sampled(obj):\n","    return tuple(obj2arrays(obj))\n","\n","replay = ReplayBuffer(2)\n","obj1 = (0, 1, 2, 3, True)\n","obj2 = (4, 5, 6, 7, False)\n","replay.add(*obj1)\n","assert replay.sample(1) == obj2sampled(obj1), \\\n","    \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n","replay.add(*obj2)\n","assert len(replay) == 2, \"Please make sure __len__ methods works as intended.\"\n","replay.add(*obj2)\n","assert len(replay) == 2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n","assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj2)\n","replay.add(*obj1)\n","assert max(len(np.unique(a)) for a in replay.sample(100)) == 2\n","replay.add(*obj1)\n","assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj1)\n","print(\"Success!\")"]},{"cell_type":"markdown","metadata":{"id":"BJUmCMq3vG74"},"source":["Now let's use this buffer to improve training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqaBudK4vG74"},"outputs":[],"source":["env = gym.make(\"Taxi-v3\")\n","n_actions = env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWRICWp5vG75"},"outputs":[],"source":["def play_and_train_with_replay(env, agent, replay=None,\n","                               t_max=10**4, replay_batch_size=32):\n","    \"\"\"\n","    This function should \n","    - run a full game, actions given by agent.get_action(s)\n","    - train agent using agent.update(...) whenever possible\n","    - return total reward\n","    :param replay: ReplayBuffer where agent can store and sample (s,a,r,s',done) tuples.\n","        If None, do not use experience replay\n","    \"\"\"\n","    total_reward = 0.0\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # get agent to pick action given state s\n","        a = <YOUR CODE>\n","\n","        next_s, r, done, _ = env.step(a)\n","\n","        # update agent on current transition. Use agent.update\n","        <YOUR CODE>\n","\n","        if replay is not None:\n","            # store current <s,a,r,s'> transition in buffer\n","            <YOUR CODE>\n","\n","            # sample replay_batch_size random transitions from replay,\n","            # then update agent on each of them in a loop\n","            s_, a_, r_, next_s_, done_ = replay.sample(replay_batch_size)\n","            for i in range(replay_batch_size):\n","                <YOUR CODE>\n","\n","        s = next_s\n","        total_reward += r\n","        if done:\n","            break\n","\n","    return total_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HAJWaZnvG75"},"outputs":[],"source":["# Create two agents: first will use experience replay, second will not.\n","\n","agent_baseline = QLearningAgent(\n","    alpha=0.5, epsilon=0.25, discount=0.99,\n","    get_legal_actions=lambda s: range(n_actions))\n","\n","agent_replay = QLearningAgent(\n","    alpha=0.5, epsilon=0.25, discount=0.99,\n","    get_legal_actions=lambda s: range(n_actions))\n","\n","replay = ReplayBuffer(1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKKyWXfevG76"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","rewards_replay, rewards_baseline = [], []\n","\n","for i in range(1000):\n","    rewards_replay.append(\n","        play_and_train_with_replay(env, agent_replay, replay))\n","    rewards_baseline.append(\n","        play_and_train_with_replay(env, agent_baseline, replay=None))\n","\n","    agent_replay.epsilon *= 0.99\n","    agent_baseline.epsilon *= 0.99\n","\n","    if i % 100 == 0:\n","        clear_output(True)\n","        print('Baseline : eps =', agent_replay.epsilon,\n","              'mean reward =', np.mean(rewards_baseline[-10:]))\n","        print('ExpReplay: eps =', agent_baseline.epsilon,\n","              'mean reward =', np.mean(rewards_replay[-10:]))\n","        plt.plot(moving_average(rewards_replay), label='exp. replay')\n","        plt.plot(moving_average(rewards_baseline), label='baseline')\n","        plt.grid()\n","        plt.legend()\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"oEgirk2evG76"},"source":["#### What to expect:\n","\n","Experience replay, if implemented correctly, will improve algorithm's initial convergence a lot, but it shouldn't affect the final performance."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"3_qlearning_sarsa.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}